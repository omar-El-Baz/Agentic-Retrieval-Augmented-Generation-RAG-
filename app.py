# --- START OF FILE app.py ---

import streamlit as st
import os
import json
import uuid # Added for potential future use if regenerating IDs in-app
from datetime import datetime # For timestamp in chat
from dotenv import load_dotenv # For .env file

# --- Load Environment Variables (especially OPENAI_API_KEY) ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("CRITICAL: OPENAI_API_KEY environment variable not set (or .env file not found/configured). The chatbot cannot function.")
    st.stop()

# --- Backend Logic (Simplified and adapted for Streamlit from the notebook) ---

# --- Member A: Data Ingestion & Vector DB Setup (Adapted for Streamlit) ---
@st.cache_resource # Cache heavy resources
def initialize_data_and_vector_db():
    import pandas as pd
    import nltk
    from sentence_transformers import SentenceTransformer
    from qdrant_client import QdrantClient, models as qdrant_models

    # NLTK downloads (silent if already present)
    try: nltk.data.find('tokenizers/punkt')
    except nltk.downloader.DownloadError: nltk.download('punkt', quiet=True) # Use nltk.download instead of nltk.downloader.DownloadError
    except LookupError: nltk.download('punkt', quiet=True) # Also catch LookupError
    
    try: nltk.data.find('tokenizers/punkt_tab')
    except nltk.downloader.DownloadError: nltk.download('punkt_tab', quiet=True)
    except LookupError: nltk.download('punkt_tab', quiet=True)


    st.write("Initializing embedding model...")
    embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
    st.write("Embedding model initialized.")

    st.write("Initializing Qdrant client (in-memory)...")
    qdrant_client_instance = QdrantClient(":memory:") # In-memory for this app
    collection_name = "streamlit_travel_data_v2" # Slightly different name to avoid cache issues if structure changes
    processed_docs_path = "processed_docs.json"
    st.write("Qdrant client initialized.")

    documents_for_indexing = []
    if os.path.exists(processed_docs_path):
        try:
            with open(processed_docs_path, "r", encoding="utf-8") as f:
                documents_for_indexing = json.load(f)
            print(f"Loaded {len(documents_for_indexing)} documents from {processed_docs_path}")
            st.success(f"Successfully loaded {len(documents_for_indexing)} pre-processed documents.")
            # Ensure the loaded documents have UUIDs for IDs.
            # This file MUST be generated by the notebook version that uses uuid.uuid4() for doc['id'].
        except Exception as e:
            print(f"Error loading {processed_docs_path}: {e}")
            st.warning(f"Could not load pre-processed documents from {processed_docs_path}: {e}. RAG might not be effective.")
            documents_for_indexing = [] # Ensure it's empty if loading fails
    else:
        st.warning(f"{processed_docs_path} not found. RAG will not have data. Please run the data processing notebook (m1.ipynb) first to generate this file with UUIDs for document IDs.")
        documents_for_indexing = []


    if documents_for_indexing:
        st.write(f"Preparing to index {len(documents_for_indexing)} documents...")
        vector_size = embedding_model.get_sentence_embedding_dimension()
        try:
            # Using newer Qdrant client approach (simplified for recreate)
            if qdrant_client_instance.collection_exists(collection_name=collection_name):
                 qdrant_client_instance.delete_collection(collection_name=collection_name)
            
            qdrant_client_instance.create_collection(
                collection_name=collection_name,
                vectors_config=qdrant_models.VectorParams(size=vector_size, distance=qdrant_models.Distance.COSINE)
            )
            st.write(f"Qdrant collection '{collection_name}' created/recreated.")

            content_list = [doc.get("content", "") for doc in documents_for_indexing] # Ensure "content" key exists
            
            # Filter out any documents that might be malformed or missing content
            valid_docs_for_embedding = [(doc, content) for doc, content in zip(documents_for_indexing, content_list) if content]
            
            if not valid_docs_for_embedding:
                st.warning("No valid documents with content found for embedding after filtering.")
            else:
                st.write(f"Generating embeddings for {len(valid_docs_for_embedding)} valid documents...")
                
                # Separate valid documents and their content for encoding
                final_docs_to_index = [doc_tuple[0] for doc_tuple in valid_docs_for_embedding]
                final_content_list = [doc_tuple[1] for doc_tuple in valid_docs_for_embedding]

                embeddings = embedding_model.encode(final_content_list, show_progress_bar=False) # No progress bar in st
                st.write("Embeddings generated.")
                
                points_to_upsert = [
                    qdrant_models.PointStruct(
                        id=doc["id"], # This ID MUST be a UUID string from processed_docs.json
                        vector=embeddings[i].tolist(),
                        payload={"text_content": doc.get("content",""), **doc.get("metadata",{})} # Use "text_content" for consistency
                    )
                    for i, doc in enumerate(final_docs_to_index) # Use final_docs_to_index
                ]

                if points_to_upsert:
                    qdrant_client_instance.upsert(collection_name=collection_name, points=points_to_upsert, wait=True)
                    print(f"Indexed {len(points_to_upsert)} documents into Qdrant collection '{collection_name}'.")
                    st.success(f"Successfully indexed {len(points_to_upsert)} documents.")
                else:
                    print("No points to upsert into Qdrant.")
                    st.info("No documents were suitable for indexing after filtering.")

        except Exception as e:
            st.error(f"Error setting up Qdrant collection or indexing: {e}")
            # In case of error, ensure clients are None so subsequent checks fail gracefully
            return None, None, None 

    # If documents_for_indexing was empty from the start
    elif not documents_for_indexing and os.path.exists(processed_docs_path):
        st.info(f"{processed_docs_path} was loaded but found to be empty or contained no valid content for indexing.")


    return embedding_model, qdrant_client_instance, collection_name

# Initialize and get components
st.spinner("Initializing data and vector database... This may take a moment.")
embedding_model_st, qdrant_client_st, qdrant_collection_name_st = initialize_data_and_vector_db()

if not all([embedding_model_st, qdrant_client_st, qdrant_collection_name_st]):
    st.error("Failed to initialize critical RAG components (embedding model or vector DB). Chatbot RAG features will be disabled.")
    # Not stopping the app, but RAG won't work. The CrewAI might still work without RAG if not dependent on the tool.

def search_vector_db_st(query_text: str, top_k: int = 3) -> list[dict]:
    if not all([embedding_model_st, qdrant_client_st, qdrant_collection_name_st]):
        print("Search components (embedding model or Qdrant client) not ready in Streamlit app.")
        return []
    if not query_text or not isinstance(query_text, str) or not query_text.strip():
        print("Invalid query text for vector DB search.")
        return []
    
    try:
        query_embedding = embedding_model_st.encode([query_text])[0]
    except Exception as e:
        print(f"Error encoding query in search_vector_db_st: {e}")
        return []

    try:
        # Using client.query which is a more common alias now for search operations
        # The full new syntax is client.query_points, but client.query often works as a simpler form.
        search_results_qdrant = qdrant_client_st.query(
            collection_name=qdrant_collection_name_st,
            query_vector=query_embedding.tolist(),
            limit=top_k,
            with_payload=True
        )
        formatted_results = []
        for hit in search_results_qdrant: # search_results_qdrant is directly iterable if it's a list of ScoredPoint
            payload = hit.payload if hit.payload else {}
            formatted_results.append({
                "id": str(hit.id),
                "score": float(hit.score),
                "content": payload.get("text_content", ""), # Standardized to "text_content"
                "metadata": {k: v for k, v in payload.items() if k != "text_content"}
            })
        return formatted_results
    except Exception as e:
        print(f"Error during Qdrant search in Streamlit app: {e}")
        st.warning(f"RAG search failed: {e}") # Inform user on UI as well
        return []

# --- Member B: CrewAI Agent and Crew Setup (Adapted for Streamlit) ---
@st.cache_resource # Cache CrewAI components
def initialize_crewai_components():
    from crewai import Agent, Task, Crew, Process
    from crewai.tools import BaseTool # Corrected import
    from langchain_openai import ChatOpenAI

    st.write("Initializing CrewAI LLM...")
    llm = ChatOpenAI(model_name="gpt-3.5-turbo-0125", temperature=0.2, openai_api_key=OPENAI_API_KEY)
    st.write("CrewAI LLM initialized.")

    class VectorDBQueryToolApp(BaseTool):
        name: str = "Travel Information Query Tool"
        description: str = "Queries a vector database for travel-related information like hotel reviews or itinerary details based on a user query. Input should be the user's query string."
        
        def _run(self, user_query: str) -> str:
            if not all([embedding_model_st, qdrant_client_st]): # Check if RAG components are ready
                return "Vector database is not available for search at the moment."
            
            print(f"[VectorDBQueryToolApp._run] Received query for DB search: '{user_query}'")
            results = search_vector_db_st(query_text=user_query, top_k=3)
            if not results: return "No relevant information snippets were found in the database for this query."
            
            output_str = "Retrieved Information Snippets:\n"
            for i, res in enumerate(results):
                # Ensure content exists before slicing
                content_preview = res.get('content', 'N/A')[:150] + "..." if res.get('content') else 'N/A'
                output_str += f"Snippet {i+1} (Score: {res.get('score', 0.0):.2f}, Source: {res.get('metadata', {}).get('source','N/A')}): {content_preview}\n"
            return output_str

    st.write("Initializing CrewAI Tools and Agents...")
    vector_db_tool_instance_st = VectorDBQueryToolApp()

    retriever_agent = Agent( # Renamed variable to avoid conflict
        role='Travel Information Retriever',
        goal='Use the "Travel Information Query Tool" to find data relevant to the user\'s query.',
        backstory='You are an AI assistant that efficiently queries a travel database.',
        tools=[vector_db_tool_instance_st], llm=llm, verbose=False, memory=False, allow_delegation=False
    )
    summarizer_agent = Agent( # Renamed variable
        role='Summary Extractor',
        goal='Condense the retrieved information into a concise summary relevant to the user query.',
        backstory='You are an AI assistant skilled at summarizing text for clarity.',
        llm=llm, verbose=False, memory=False, allow_delegation=False
    )
    composer_agent = Agent( # Renamed variable
        role='Travel Advisor',
        goal='Formulate a helpful and friendly travel recommendation or answer based on the provided summary.',
        backstory='You are an AI travel assistant that provides engaging and practical advice.',
        llm=llm, verbose=False, memory=False, allow_delegation=False
    )
    st.write("CrewAI components initialized.")
    print("CrewAI components initialized for Streamlit app.")
    return retriever_agent, summarizer_agent, composer_agent, llm

# Initialize and get CrewAI components
st.spinner("Initializing AI Agents... This may take a moment.")
crew_retriever_st, crew_summarizer_st, crew_composer_st, crew_llm_st = initialize_crewai_components()

if not all([crew_retriever_st, crew_summarizer_st, crew_composer_st, crew_llm_st]):
    st.error("Failed to initialize CrewAI components. Chatbot functionality will be limited.")

def get_travel_recommendation_streamlit(user_query: str) -> str:
    from crewai import Task, Crew, Process # Keep imports local to function if not used elsewhere globally
    
    if not all([crew_retriever_st, crew_summarizer_st, crew_composer_st]):
        return "Error: CrewAI agents are not ready. Please check the application logs."
    if not user_query or not user_query.strip(): 
        return "Please enter a query."

    retrieval_task = Task(
        description=f"The user's query is: '{user_query}'. Use your tool to find relevant travel information from the database.",
        expected_output="A string containing relevant snippets from the travel database, or a message if no information is found.",
        agent=crew_retriever_st # Use Streamlit specific agent variable
    )
    summarization_task = Task(
        description=f"Based on the retrieved snippets for the query '{user_query}', create a concise summary. If no snippets were found, state that no information was available.",
        expected_output="A short, factual summary of the key information, or a statement that no information was found.",
        agent=crew_summarizer_st, context=[retrieval_task]
    )
    composition_task = Task(
        description=f"Using the summary for the query '{user_query}', compose a friendly and helpful travel recommendation. If the summary indicates no information was found, politely inform the user.",
        expected_output="A final, well-formatted response for the user.",
        agent=crew_composer_st, context=[summarization_task]
    )
    travel_crew = Crew(
        agents=[crew_retriever_st, crew_summarizer_st, crew_composer_st],
        tasks=[retrieval_task, summarization_task, composition_task],
        process=Process.sequential, 
        verbose=False # Changed from 0 to False for boolean type; can also be 1 for True
    )
    try:
        st.write("🤖 CrewAI is processing your request...")
        result = travel_crew.kickoff()
        return str(result)
    except Exception as e:
        print(f"Error during CrewAI kickoff in Streamlit: {e}")
        st.error(f"Sorry, I encountered an issue processing your request. Details: {str(e)}")
        return f"Sorry, I encountered an issue processing your request."

# --- Streamlit UI Configuration ---
st.set_page_config(page_title="AI Travel Guide", layout="wide", initial_sidebar_state="expanded") # Expanded sidebar
st.title("🌍 AI Travel Guide Chatbot")
st.caption("Your personal AI travel assistant. Ask me anything about your travel plans!")

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Hello! How can I assist with your travel plans today?"}]

# Sidebar for Info & Controls
with st.sidebar:
    st.header("Chat Controls")
    if st.button("Clear Chat History"):
        st.session_state.messages = [{"role": "assistant", "content": "Hello! How can I assist with your travel plans today?"}]
        st.rerun()
    st.markdown("---")
    st.header("About")
    st.markdown(
        "This chatbot uses a Retrieval-Augmented Generation (RAG) approach with CrewAI "
        "to provide travel recommendations based on a knowledge base of hotel reviews. "
        "Ensure `processed_docs.json` (with UUIDs for IDs) is present in the app directory."
    )
    st.markdown("---")
    st.subheader("System Status")
    if OPENAI_API_KEY.startswith("sk-") and len(OPENAI_API_KEY) > 50: # Basic check
        st.success("OpenAI API Key: Loaded")
    else:
        st.error("OpenAI API Key: Invalid or missing")

    if embedding_model_st:
        st.success("Embedding Model: Initialized")
    else:
        st.warning("Embedding Model: Not initialized")
        
    if qdrant_client_st and qdrant_collection_name_st:
        try:
            # Use get_collection to check existence and get info
            collection_info = qdrant_client_st.get_collection(collection_name=qdrant_collection_name_st)
            st.success(f"Vector DB: Connected. Indexed: {collection_info.points_count}")
        except Exception as e: # More specific exception might be qdrant_client.http.exceptions.UnexpectedResponse
            if "not found" in str(e).lower() or "status_code=404" in str(e).lower():
                 st.warning(f"Vector DB: Collection '{qdrant_collection_name_st}' not found or empty.")
            else:
                st.warning(f"Vector DB: Status unknown ({e})")
    else:
        st.warning("Vector DB: Not initialized")

    if crew_retriever_st and crew_summarizer_st and crew_composer_st and crew_llm_st:
        st.success("CrewAI Agents: Initialized")
    else:
        st.warning("CrewAI Agents: Not fully initialized")
    
    st.markdown("---")
    st.markdown("Powered by [Streamlit](https://streamlit.io/) & [CrewAI](https://www.crewai.com/)")


# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("e.g., Find a luxury hotel in Paris near Eiffel Tower with a spa"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("🤖 Assistant is thinking... This might take a few moments..."):
            if not all([crew_retriever_st, crew_summarizer_st, crew_composer_st, crew_llm_st]):
                 response = "I'm sorry, my AI components are not fully initialized. Please check the console logs or application status in the sidebar."
            elif not all([embedding_model_st, qdrant_client_st]) and "hotel" in prompt.lower(): # Basic check if RAG might be needed
                 response = "My document search capabilities are not ready, but I can try to answer generally. " + get_travel_recommendation_streamlit(prompt)
            else:
                response = get_travel_recommendation_streamlit(prompt)
        
        message_placeholder.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
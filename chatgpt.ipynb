{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ec290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n",
      "2.19.0\n",
      "3.9.2\n",
      "2.19.0\n",
      "OPENAI_API_KEY loaded successfully (from environment or .env file).\n",
      "NLTK 'punkt' resource found.\n",
      "NLTK 'punkt_tab' resource found.\n",
      "Dataset 'tripadvisor_hotel_reviews.csv' loaded successfully. Shape: (20491, 2)\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20491 entries, 0 to 20490\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  20491 non-null  object\n",
      " 1   Rating  20491 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 320.3+ KB\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                                              Review  Rating\n",
      "0  nice hotel expensive parking got good deal sta...       4\n",
      "1  ok nothing special charge diamond member hilto...       2\n",
      "2  nice rooms not 4* experience hotel monaco seat...       3\n",
      "3  unique, great stay, wonderful time hotel monac...       5\n",
      "4  great stay great stay, went seahawk game aweso...       5\n",
      "\n",
      "Cleaning reviews...\n",
      "Cleaned reviews (sample of original vs cleaned):\n",
      "                                              Review  \\\n",
      "0  nice hotel expensive parking got good deal sta...   \n",
      "1  ok nothing special charge diamond member hilto...   \n",
      "2  nice rooms not 4* experience hotel monaco seat...   \n",
      "3  unique, great stay, wonderful time hotel monac...   \n",
      "4  great stay great stay, went seahawk game aweso...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  nicehotelexpensiveparkinggotgooddealstayhotela...  \n",
      "1  oknothingspecialchargediamondmemberhiltondecid...  \n",
      "2  niceroomsnot4experiencehotelmonacoseattlegoodh...  \n",
      "3  unique,greatstay,wonderfultimehotelmonaco,loca...  \n",
      "4  greatstaygreatstay,wentseahawkgameawesome,down...  \n",
      "\n",
      "Creating document chunks with metadata...\n",
      "Created 20494 document chunks for indexing.\n",
      "Example document structure:\n",
      "{\n",
      "  \"id\": \"a024b936-fdfb-403e-8156-7ebe11684943\",\n",
      "  \"content\": \"nicehotelexpensiveparkinggotgooddealstayhotelanniversary,arrivedlateeveningtookadvicepreviousreviewsdidvaletparking,checkquickeasy,littledisappointednonexistentviewroomroomcleannicesize,bedcomfortablewokestiffneckhighpillows,notsoundprooflikeheardmusicroomnightmorningloudbangsdoorsopeningclosinghearpeopletalkinghallway,maybejustnoisyneighbors,avedabathproductsnice,didnotgoldfishstaynicetouchtakenadvantagestayinglonger,locationgreatwalkingdistanceshopping,overallniceexperiencehavingpay40parkingnight,\",\n",
      "  \"metadata\": {\n",
      "    \"original_review_id\": 0,\n",
      "    \"source\": \"tripadvisor_review_0\",\n",
      "    \"chunk_sequential_id_in_review\": 0,\n",
      "    \"rating\": 4,\n",
      "    \"timestamp_processed\": \"2025-05-30T11:19:11.714970\",\n",
      "    \"category\": \"hotel_review\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Saving processed documents to processed_docs.json and processed_docs.csv...\n",
      "Successfully saved to processed_docs.json\n",
      "Successfully saved to processed_docs.csv\n",
      "\n",
      "Setting up vector indexing...\n",
      "Embedding model 'all-MiniLM-L6-v2' loaded.\n",
      "Qdrant client initialized (in-memory).\n",
      "Detected vector size for embeddings: 384\n",
      "Qdrant collection 'travel_guide_rag_collection_v2' created/recreated.\n",
      "Generating embeddings for 20494 content chunks... (This may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/jv928wvn2gv7wnyyzcr4lmch0000gn/T/ipykernel_36496/4136927803.py:299: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client_instance.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a683f1983e482e9534e0189a169ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated.\n",
      "Upserting 20494 points to Qdrant collection 'travel_guide_rag_collection_v2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/jv928wvn2gv7wnyyzcr4lmch0000gn/T/ipykernel_36496/4136927803.py:325: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 20494 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  qdrant_client_instance.upsert(collection_name=qdrant_collection_name, points=points_to_upsert, wait=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert complete.\n",
      "Collection 'travel_guide_rag_collection_v2' now contains 20494 points.\n",
      "`search_vector_db` function defined for retrieving documents.\n",
      "\n",
      "--- Testing `search_vector_db` function ---\n",
      "Test Query: \"hotel with excellent spa facilities and city view\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/jv928wvn2gv7wnyyzcr4lmch0000gn/T/ipykernel_36496/4136927803.py:364: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_hits = qdrant_client_instance.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results for test query:\n",
      "  Result 1: ID: c7dd1a80-e4e2-48c4-8c02-2cccaa66ea4c, Score: 0.6264\n",
      "    Content (snippet): greatplace,hotelstreetfrancatrainstationgreattravellingtrain,closemaincentrebeachesshopssiteswalkingdistance,hotelcleane...\n",
      "    Metadata (source): tripadvisor_review_10146\n",
      "  Result 2: ID: c9505d1e-14d7-49fd-9bb5-5478a4b92d54, Score: 0.6082\n",
      "    Content (snippet): bestbestreasonaffordablebetterhotelscityseattle,greattimestaygreatservicefriendlyemployees,locationconvenientparkingchea...\n",
      "    Metadata (source): tripadvisor_review_20473\n",
      "OpenAI LLM for CrewAI initialized (gpt-3.5-turbo-0125).\n",
      "\n",
      "`search_vector_db` function from Member A's work is available.\n",
      "VectorDBQueryToolForCrew (CrewAI tool) created successfully.\n",
      "\n",
      "--- Testing the CrewAI Tool directly ---\n",
      "Tool Test Query: 'any good hotels in downtown with a gym and free breakfast?'\n",
      "Using Tool: Travel Information Vector Database Query Tool\n",
      "[VectorDBQueryToolForCrew._run] Received query for DB search: 'any good hotels in downtown with a gym and free breakfast?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/jv928wvn2gv7wnyyzcr4lmch0000gn/T/ipykernel_36496/4136927803.py:364: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_hits = qdrant_client_instance.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Retrieved Info (first 600 chars):\n",
      "Retrieved Information Snippets (Format: [Source, Rating, Relevance Score] Content):\n",
      "Snippet 1: [tripadvisor_review_4248, Rating: 5, Score: 0.517] greathotellocationstayednightweekaugust,hotelcleanbeautiful,locationgreatrighteatoncentre,easyconnectionsubwayeasywalkdowntown,parkcarforgetit.alittlepriceyvacationerroom240taxesbeatsstayingoutsidecitybuckingtrafficmakingquestionablereservationhotelcitynotfeelsecure.go,...\n",
      "---\n",
      "Snippet 2: [tripadvisor_review_2368, Rating: 5, Score: 0.506] greathotel,stayed5daybreak,roomtowersgreatview,hotelstaffpoliteroomscleangreatlocation,...\n",
      "---\n",
      "Snippet 3: [tripadv...\n",
      "\n",
      "Defining CrewAI agents...\n",
      "CrewAI Retriever Agent defined.\n",
      "CrewAI Summarizer Agent defined.\n",
      "CrewAI Composer Agent defined.\n",
      "`get_travel_recommendation_crewai` function defined and ready for use.\n",
      "\n",
      "--- Running CrewAI Demo with Example Query 1 ---\n",
      "\n",
      "--- Initiating CrewAI Process for Query: 'I want to find a quiet, charming boutique hotel in Paris, preferably in the Latin Quarter or Marais, with good reviews for cleanliness.' ---\n",
      "\n",
      "Kicking off the CrewAI travel recommendation process...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────── Crew Execution Started ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">c8e71e6b-b48d-444e-84ab-fda279d9244f</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────────\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36m────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36mc8e71e6b-b48d-444e-84ab-fda279d9244f\u001b[0m                                                                       \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\"># Agent:</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Travel Information Retrieval Specialist</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;95m# Agent:\u001b[0m \u001b[1;92mTravel Information Retrieval Specialist\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">## Task:</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">A user is asking the following travel-related question: 'I want to find a quiet, charming boutique hotel </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">in Paris, preferably in the Latin Quarter or Marais, with good reviews for cleanliness.'. Your primary objective is</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">to use the 'Travel Information Vector Database Query Tool' by providing it with this exact query: 'I want to find a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">quiet, charming boutique hotel in Paris, preferably in the Latin Quarter or Marais, with good reviews for </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">cleanliness.'. Ensure you extract all relevant text snippets from the database that could help answer this query.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[95m## Task:\u001b[0m \u001b[92mA user is asking the following travel-related question: 'I want to find a quiet, charming boutique hotel \u001b[0m\n",
       "\u001b[92min Paris, preferably in the Latin Quarter or Marais, with good reviews for cleanliness.'. Your primary objective is\u001b[0m\n",
       "\u001b[92mto use the 'Travel Information Vector Database Query Tool' by providing it with this exact query: 'I want to find a\u001b[0m\n",
       "\u001b[92mquiet, charming boutique hotel in Paris, preferably in the Latin Quarter or Marais, with good reviews for \u001b[0m\n",
       "\u001b[92mcleanliness.'. Ensure you extract all relevant text snippets from the database that could help answer this query.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9641f70d7604847aae9ebf0d9558efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard library for interacting with the operating system, like getting environment variables.\n",
    "import os\n",
    "# Used to load environment variables from a .env file into os.environ.\n",
    "from dotenv import load_dotenv\n",
    "# Provides access to system-specific parameters and functions, like the Python interpreter path.\n",
    "import sys\n",
    "# For generating universally unique identifiers, used for Qdrant point IDs.\n",
    "import uuid\n",
    "\n",
    "# Print the path to the Python interpreter being used by this notebook. Useful for debugging environment issues.\n",
    "print(sys.executable)\n",
    "\n",
    "# Import TensorFlow and print its version. TensorFlow is a machine learning framework.\n",
    "# It's a dependency for sentence-transformers, even if PyTorch is the primary backend.\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "# Import Keras and print its version. Keras is a high-level API for building and training neural networks.\n",
    "# It can run on top of TensorFlow.\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "# Import tf_keras (a Keras 2 API shim for TensorFlow) and print its version.\n",
    "# This helps with compatibility between Keras 3 and libraries expecting Keras 2.\n",
    "import tf_keras\n",
    "print(tf_keras.__version__)\n",
    "\n",
    "# Load variables from a .env file in the current directory into environment variables.\n",
    "# This is typically used for sensitive information like API keys.\n",
    "load_dotenv()\n",
    "# Retrieve the OpenAI API key from the environment variables.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the OpenAI API key was successfully loaded.\n",
    "if not OPENAI_API_KEY:\n",
    "    # If not found, print a warning message with instructions.\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment variables or .env file.\")\n",
    "    print(\"CrewAI agents (Member B's part) will not function correctly without it.\")\n",
    "    print(\"Please create a .env file in the root directory with: OPENAI_API_KEY='sk-your_key_here'\")\n",
    "else:\n",
    "    # If found, confirm it was loaded.\n",
    "    print(\"OPENAI_API_KEY loaded successfully (from environment or .env file).\")\n",
    "\n",
    "\n",
    "# Cell 2: Data Ingestion & NLTK Setup Imports\n",
    "\n",
    "# Pandas for data manipulation and analysis, especially for working with CSV files and DataFrames.\n",
    "import pandas as pd\n",
    "# Natural Language Toolkit for text processing tasks like tokenization.\n",
    "import nltk\n",
    "# Regular expression operations for text pattern matching and manipulation.\n",
    "import re\n",
    "# BeautifulSoup for parsing HTML and XML documents, used here to clean text.\n",
    "from bs4 import BeautifulSoup\n",
    "# Datetime module for working with dates and times, used for timestamping.\n",
    "from datetime import datetime\n",
    "# JSON library for working with JSON data.\n",
    "import json\n",
    "# os was already imported but often re-listed if a cell is standalone.\n",
    "\n",
    "# Function to download NLTK resources if they are not already present.\n",
    "def download_nltk_resource(resource_name, resource_path):\n",
    "    try:\n",
    "        # Try to find the NLTK resource (e.g., 'tokenizers/punkt').\n",
    "        nltk.data.find(resource_path)\n",
    "        print(f\"NLTK '{resource_name}' resource found.\")\n",
    "    except LookupError:\n",
    "        # If not found, download it.\n",
    "        print(f\"NLTK '{resource_name}' resource not found. Downloading...\")\n",
    "        nltk.download(resource_name, quiet=True) # quiet=True suppresses verbose download output.\n",
    "        print(f\"NLTK '{resource_name}' downloaded.\")\n",
    "    except Exception as e:\n",
    "        # Handle any other errors during the download process.\n",
    "        print(f\"Error checking/downloading NLTK '{resource_name}': {e}\")\n",
    "\n",
    "# Download 'punkt' tokenizer models (used for sentence tokenization).\n",
    "download_nltk_resource('punkt', 'tokenizers/punkt')\n",
    "# Download 'punkt_tab' (additional data for punkt, often for specific languages/cases, though not always strictly needed).\n",
    "download_nltk_resource('punkt_tab', 'tokenizers/punkt_tab')\n",
    "\n",
    "\n",
    "# Cell 3: Load Dataset\n",
    "\n",
    "# Define the path to the dataset CSV file.\n",
    "dataset_path = \"tripadvisor_hotel_reviews.csv\"\n",
    "# Initialize an empty Pandas DataFrame. This will be populated if the file is found.\n",
    "df = pd.DataFrame() \n",
    "\n",
    "# Check if the dataset file exists at the specified path.\n",
    "if not os.path.exists(dataset_path):\n",
    "    # If not, print an error and instructions on how to get it.\n",
    "    print(f\"ERROR: Dataset file not found at {dataset_path}\")\n",
    "    print(\"Please download it from Kaggle (e.g., https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews) and place it in the same directory as this notebook.\")\n",
    "else:\n",
    "    # If the file exists, try to read it into a Pandas DataFrame.\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        print(f\"Dataset '{dataset_path}' loaded successfully. Shape: {df.shape}\")\n",
    "        # If the DataFrame is not empty after loading:\n",
    "        if not df.empty:\n",
    "          print(\"Dataset Info:\")\n",
    "          df.info() # Display a concise summary of the DataFrame (column types, non-null values).\n",
    "          print(\"\\nFirst 5 rows of the dataset:\")\n",
    "          print(df.head()) # Display the first 5 rows.\n",
    "    except Exception as e:\n",
    "        # Handle any errors during file loading.\n",
    "        print(f\"Error loading dataset '{dataset_path}': {e}\")\n",
    "\n",
    "\n",
    "# Cell 4: Preprocessing - Clean and Chunk Text\n",
    "\n",
    "# Function to clean text data.\n",
    "def clean_text(text):\n",
    "    # Ensure the input is a string. If not, return an empty string.\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" \n",
    "    # Use BeautifulSoup to remove HTML tags.\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  \n",
    "    # Remove characters that are not alphanumeric, whitespace, or basic punctuation.\n",
    "    # Double backslashes are needed for regex in Python strings, especially for '\\s', '\\'', '\\\"'.\n",
    "    text = re.sub(r'[^A-Za-z0-9\\\\s,.!?\\\\\\'\\\"]', '', text)\n",
    "    # Replace multiple whitespace characters with a single space and strip leading/trailing whitespace.\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Check if the DataFrame is not empty and contains the 'Review' column.\n",
    "if not df.empty and 'Review' in df.columns:\n",
    "    print(\"\\nCleaning reviews...\")\n",
    "    # Apply the clean_text function to each review in the 'Review' column.\n",
    "    df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "    print(\"Cleaned reviews (sample of original vs cleaned):\")\n",
    "    # Show a sample of original vs. cleaned reviews.\n",
    "    print(df[['Review', 'cleaned_review']].head())\n",
    "elif df.empty:\n",
    "    print(\"DataFrame is empty, skipping review cleaning.\")\n",
    "else:\n",
    "    print(\"Column 'Review' not found in DataFrame, skipping review cleaning.\")\n",
    "\n",
    "# Function to chunk text into smaller pieces, respecting a maximum token limit.\n",
    "def chunk_text(text, max_tokens=450):\n",
    "    # If text is not a string or is empty/whitespace only, return an empty list.\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [] \n",
    "    \n",
    "    # Tokenize the text into sentences using NLTK.\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk_sentences = [] # Accumulates sentences for the current chunk.\n",
    "    current_token_count = 0      # Tracks token count for the current chunk.\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the current sentence into words to count tokens.\n",
    "        sentence_tokens = nltk.word_tokenize(sentence) \n",
    "        token_count_for_sentence = len(sentence_tokens)\n",
    "        \n",
    "        # If a single sentence is already larger than max_tokens, split it.\n",
    "        if token_count_for_sentence > max_tokens: \n",
    "            # If there's an existing chunk being built, finalize it.\n",
    "            if current_chunk_sentences: \n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = []\n",
    "                current_token_count = 0\n",
    "            \n",
    "            # Split the oversized sentence into sub-chunks of max_tokens.\n",
    "            start = 0\n",
    "            while start < token_count_for_sentence:\n",
    "                sub_sentence_tokens = sentence_tokens[start : start + max_tokens]\n",
    "                chunks.append(\" \".join(sub_sentence_tokens)) \n",
    "                start += max_tokens\n",
    "            continue # Move to the next sentence.\n",
    "\n",
    "        # If adding the current sentence doesn't exceed max_tokens for the current chunk:\n",
    "        if current_token_count + token_count_for_sentence <= max_tokens:\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_token_count += token_count_for_sentence\n",
    "        # If it would exceed max_tokens:\n",
    "        else: \n",
    "            # Finalize the current chunk if it has content.\n",
    "            if current_chunk_sentences: \n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "            # Start a new chunk with the current sentence.\n",
    "            current_chunk_sentences = [sentence] \n",
    "            current_token_count = token_count_for_sentence\n",
    "            \n",
    "    # Add any remaining sentences in the last chunk.\n",
    "    if current_chunk_sentences: \n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "        \n",
    "    # Return only non-empty chunks.\n",
    "    return [chunk for chunk in chunks if chunk.strip()]\n",
    "\n",
    "\n",
    "# Cell 5: Create Documents with Metadata & Save\n",
    "\n",
    "# List to store the processed documents for indexing.\n",
    "documents_for_indexing = []\n",
    "# Ensure DataFrame has necessary columns.\n",
    "if not df.empty and 'cleaned_review' in df.columns and 'Rating' in df.columns:\n",
    "    print(\"\\nCreating document chunks with metadata...\")\n",
    "    # Iterate over each row in the DataFrame.\n",
    "    for idx, row in df.iterrows():\n",
    "        cleaned_review_text = row['cleaned_review']\n",
    "        original_review_identifier = idx # Use DataFrame index as an identifier for the original review.\n",
    "        try:\n",
    "            # Attempt to convert rating to an integer.\n",
    "            rating_val = int(row[\"Rating\"])\n",
    "        except (ValueError, TypeError):\n",
    "            # If conversion fails, default to 0 or handle as appropriate.\n",
    "            rating_val = 0 \n",
    "\n",
    "        # Chunk the cleaned review text.\n",
    "        chunks = chunk_text(cleaned_review_text)\n",
    "        for i, chunk_content in enumerate(chunks):\n",
    "            # Skip empty chunks.\n",
    "            if not chunk_content.strip(): continue\n",
    "            \n",
    "            # Create a unique ID for each chunk using UUID.\n",
    "            chunk_unique_id = str(uuid.uuid4()) \n",
    "            \n",
    "            # Append the chunk and its metadata to the list.\n",
    "            documents_for_indexing.append({\n",
    "                \"id\": chunk_unique_id, \n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {\n",
    "                    \"original_review_id\": original_review_identifier, \n",
    "                    \"source\": f\"tripadvisor_review_{original_review_identifier}\", # Source identifier.\n",
    "                    \"chunk_sequential_id_in_review\": i, # Order of this chunk within the original review.\n",
    "                    \"rating\": rating_val, # Associated rating.\n",
    "                    \"timestamp_processed\": datetime.now().isoformat(), # Processing timestamp.\n",
    "                    \"category\": \"hotel_review\" # Document category.\n",
    "                }\n",
    "            })\n",
    "    print(f\"Created {len(documents_for_indexing)} document chunks for indexing.\")\n",
    "    # If documents were created, show an example and save them.\n",
    "    if documents_for_indexing:\n",
    "        print(\"Example document structure:\")\n",
    "        # Print the first document as a formatted JSON string.\n",
    "        print(json.dumps(documents_for_indexing[0], indent=2))\n",
    "\n",
    "        processed_json_path = \"processed_docs.json\"\n",
    "        processed_csv_path = \"processed_docs.csv\"\n",
    "        \n",
    "        print(f\"\\nSaving processed documents to {processed_json_path} and {processed_csv_path}...\")\n",
    "        try:\n",
    "            # Save as JSON.\n",
    "            with open(processed_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(documents_for_indexing, f, indent=2)\n",
    "            print(f\"Successfully saved to {processed_json_path}\")\n",
    "\n",
    "            # Prepare data for CSV saving (flattening metadata).\n",
    "            df_to_save_data = []\n",
    "            for doc in documents_for_indexing:\n",
    "                # Combine 'id', 'content', and all metadata fields into a flat dictionary.\n",
    "                flat_doc = {\"id\": doc[\"id\"],\"content\": doc[\"content\"],**doc[\"metadata\"]}\n",
    "                df_to_save_data.append(flat_doc)\n",
    "            # Save as CSV.\n",
    "            pd.DataFrame(df_to_save_data).to_csv(processed_csv_path, index=False, encoding=\"utf-8\")\n",
    "            print(f\"Successfully saved to {processed_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving processed documents: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping document creation: DataFrame is empty or required columns ('cleaned_review', 'Rating') are missing.\")\n",
    "    documents_for_indexing = [] # Ensure it's an empty list if skipped.\n",
    "\n",
    "\n",
    "# Cell 6: Vector Indexing & Retrieval Setup\n",
    "\n",
    "# For creating text embeddings.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Qdrant client for vector database operations.\n",
    "from qdrant_client import QdrantClient, models as qdrant_models \n",
    "\n",
    "# Name for the Qdrant collection.\n",
    "qdrant_collection_name = \"travel_guide_rag_collection_v2\"\n",
    "# Initialize Qdrant client and embedding model instances as None.\n",
    "qdrant_client_instance = None \n",
    "embedding_model_instance = None \n",
    "\n",
    "# Proceed only if there are documents to index.\n",
    "if documents_for_indexing:\n",
    "    print(\"\\nSetting up vector indexing...\")\n",
    "    # Name of the sentence transformer model to use for embeddings.\n",
    "    model_name_for_embedding = \"all-MiniLM-L6-v2\"\n",
    "    try:\n",
    "        # Load the pre-trained sentence transformer model.\n",
    "        embedding_model_instance = SentenceTransformer(model_name_for_embedding)\n",
    "        print(f\"Embedding model '{model_name_for_embedding}' loaded.\")\n",
    "\n",
    "        # Initialize Qdrant client to run in-memory.\n",
    "        qdrant_client_instance = QdrantClient(\":memory:\") \n",
    "        print(\"Qdrant client initialized (in-memory).\")\n",
    "\n",
    "        # Get the dimensionality of the embeddings produced by the model.\n",
    "        vector_size = embedding_model_instance.get_sentence_embedding_dimension()\n",
    "        print(f\"Detected vector size for embeddings: {vector_size}\")\n",
    "\n",
    "        # Recreate the Qdrant collection (deletes if exists, then creates).\n",
    "        # This ensures a fresh collection for each run.\n",
    "        qdrant_client_instance.recreate_collection(\n",
    "            collection_name=qdrant_collection_name,\n",
    "            vectors_config=qdrant_models.VectorParams(size=vector_size, distance=qdrant_models.Distance.COSINE) # Using Cosine similarity.\n",
    "        )\n",
    "        print(f\"Qdrant collection '{qdrant_collection_name}' created/recreated.\")\n",
    "    \n",
    "        # Extract the content from each document for embedding.\n",
    "        content_list = [doc[\"content\"] for doc in documents_for_indexing]\n",
    "        print(f\"Generating embeddings for {len(content_list)} content chunks... (This may take a while)\")\n",
    "        # Encode the content list into embeddings. show_progress_bar displays a progress bar.\n",
    "        embeddings = embedding_model_instance.encode(content_list, show_progress_bar=True)\n",
    "        print(\"Embeddings generated.\")\n",
    "        \n",
    "        # Prepare points (documents with vectors and payloads) for upserting into Qdrant.\n",
    "        points_to_upsert = [\n",
    "            qdrant_models.PointStruct(\n",
    "                id=doc[\"id\"], # Unique ID for the point (must be UUID).\n",
    "                vector=embeddings[i].tolist(), # The embedding vector.\n",
    "                payload={\"text_content\": doc[\"content\"], **doc[\"metadata\"]} # Store content and metadata.\n",
    "            )\n",
    "            for i, doc in enumerate(documents_for_indexing)\n",
    "        ]\n",
    "        \n",
    "        # Upsert the points to the Qdrant collection if any points were created.\n",
    "        if points_to_upsert:\n",
    "            print(f\"Upserting {len(points_to_upsert)} points to Qdrant collection '{qdrant_collection_name}'...\")\n",
    "            qdrant_client_instance.upsert(collection_name=qdrant_collection_name, points=points_to_upsert, wait=True)\n",
    "            print(\"Upsert complete.\")\n",
    "            # Get collection info to verify the number of points.\n",
    "            collection_info = qdrant_client_instance.get_collection(collection_name=qdrant_collection_name)\n",
    "            print(f\"Collection '{qdrant_collection_name}' now contains {collection_info.points_count} points.\")\n",
    "        else:\n",
    "            print(\"No points generated to upsert into Qdrant.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # If any error occurs, print it and reset client/model instances.\n",
    "        print(f\"Error during vector indexing setup or embedding process: {e}\")\n",
    "        qdrant_client_instance = None \n",
    "        embedding_model_instance = None\n",
    "else:\n",
    "    print(\"\\nSkipping vector indexing as no documents were processed or loaded for indexing.\")\n",
    "\n",
    "\n",
    "# Cell 7: Retrieval Function\n",
    "\n",
    "# Function to search the vector database.\n",
    "def search_vector_db(query_text: str, top_k: int = 5) -> list[dict]:\n",
    "    # Check if Qdrant client and embedding model are initialized.\n",
    "    if not qdrant_client_instance or not embedding_model_instance:\n",
    "        print(\"ERROR in search_vector_db: Qdrant client or embedding model is not initialized. Cannot perform search.\")\n",
    "        return []\n",
    "    # Check if the query text is valid.\n",
    "    if not query_text or not isinstance(query_text, str):\n",
    "        print(\"ERROR in search_vector_db: Query text is invalid.\")\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        # Encode the query text into an embedding.\n",
    "        query_embedding = embedding_model_instance.encode([query_text])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding query text in search_vector_db: {e}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "      # Perform the search operation in Qdrant.\n",
    "      search_hits = qdrant_client_instance.search(\n",
    "          collection_name=qdrant_collection_name,\n",
    "          query_vector=query_embedding.tolist(), # The query embedding.\n",
    "          limit=top_k, # Number of results to return.\n",
    "          with_payload=True # Include the payload (metadata and content) in results.\n",
    "      )\n",
    "    except Exception as e:\n",
    "      print(f\"Error during Qdrant search operation: {e}\")\n",
    "      return []\n",
    "    \n",
    "    # Format the search results into a list of dictionaries.\n",
    "    formatted_search_results = []\n",
    "    for hit in search_hits:\n",
    "        payload = hit.payload if hit.payload else {} # Get payload, default to empty dict if None.\n",
    "        formatted_search_results.append({\n",
    "            \"id\": str(hit.id), \n",
    "            \"score\": float(hit.score), # Similarity score.\n",
    "            \"content\": payload.get(\"text_content\", \"\"), # Get the text content from payload.\n",
    "            \"metadata\": {k: v for k, v in payload.items() if k != \"text_content\"} # Get other metadata.\n",
    "        })\n",
    "    return formatted_search_results\n",
    "\n",
    "print(\"`search_vector_db` function defined for retrieving documents.\")\n",
    "\n",
    "# Test the search function if components are ready and documents were indexed.\n",
    "if qdrant_client_instance and embedding_model_instance and documents_for_indexing:\n",
    "    try:\n",
    "        collection_info_for_test = qdrant_client_instance.get_collection(collection_name=qdrant_collection_name)\n",
    "        # Only test if the collection actually has points.\n",
    "        if collection_info_for_test.points_count > 0:\n",
    "            test_search_query = \"hotel with excellent spa facilities and city view\"\n",
    "            print(f\"\\n--- Testing `search_vector_db` function ---\")\n",
    "            print(f\"Test Query: \\\"{test_search_query}\\\"\")\n",
    "            retrieved_search_results = search_vector_db(test_search_query, top_k=2)\n",
    "            if retrieved_search_results:\n",
    "                print(f\"Found {len(retrieved_search_results)} results for test query:\")\n",
    "                for i, res_item in enumerate(retrieved_search_results):\n",
    "                    print(f\"  Result {i+1}: ID: {res_item['id']}, Score: {res_item['score']:.4f}\")\n",
    "                    print(f\"    Content (snippet): {res_item['content'][:120]}...\")\n",
    "                    print(f\"    Metadata (source): {res_item['metadata'].get('source', 'N/A')}\")\n",
    "            else:\n",
    "                print(\"No results found for the test search query or the search operation failed.\")\n",
    "        else:\n",
    "            print(\"Skipping retrieval function test as Qdrant collection is empty.\")\n",
    "    except Exception as e_test_search:\n",
    "        print(f\"Could not perform retrieval function test due to an error: {e_test_search}\")\n",
    "else:\n",
    "    print(\"\\nSkipping retrieval function test: Qdrant client, embedding model, or indexed documents are not ready.\")\n",
    "\n",
    "\n",
    "# Cell 8: CrewAI & LLM Setup Imports\n",
    "\n",
    "# Core CrewAI classes for defining agents, tasks, and crews.\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "# Base class for creating custom tools for CrewAI agents.\n",
    "from crewai.tools import BaseTool # Corrected import path.\n",
    "# LangChain's OpenAI chat model wrapper.\n",
    "from langchain_openai import ChatOpenAI\n",
    "# os and OPENAI_API_KEY are assumed to be loaded from Cell 1.\n",
    "\n",
    "# Initialize LLM variable.\n",
    "llm_for_crewai = None \n",
    "\n",
    "# Check if the OpenAI API key is available.\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"ERROR: OPENAI_API_KEY environment variable not set. CrewAI agents cannot be initialized.\")\n",
    "else:\n",
    "    # If key is available, try to initialize the ChatOpenAI model.\n",
    "    try:\n",
    "        llm_for_crewai = ChatOpenAI(\n",
    "            model_name=\"gpt-3.5-turbo-0125\", # Specify the OpenAI model.\n",
    "            temperature=0.2, # Low temperature for more deterministic, less creative responses.\n",
    "            openai_api_key=OPENAI_API_KEY # Pass the API key.\n",
    "        )\n",
    "        print(\"OpenAI LLM for CrewAI initialized (gpt-3.5-turbo-0125).\")\n",
    "    except Exception as e_llm_init:\n",
    "        print(f\"Error initializing OpenAI LLM for CrewAI: {e_llm_init}. Check your API key and relevant package versions.\")\n",
    "\n",
    "\n",
    "# Cell 9: Retrieval Agent Tool Setup\n",
    "\n",
    "# Initialize the CrewAI tool variable.\n",
    "crewai_vector_db_tool = None \n",
    "try:\n",
    "    # Check if the search_vector_db function (from Member A's part) is defined.\n",
    "    search_vector_db \n",
    "    print(\"\\n`search_vector_db` function from Member A's work is available.\")\n",
    "\n",
    "    # Define a custom CrewAI tool that wraps the search_vector_db function.\n",
    "    class VectorDBQueryToolForCrew(BaseTool):\n",
    "        name: str = \"Travel Information Vector Database Query Tool\" # Name of the tool.\n",
    "        description: str = ( # Description for the LLM to understand how to use the tool.\n",
    "            \"Use this specialized tool to query the travel vector database. \"\n",
    "            \"Input MUST be the user's specific query string. \"\n",
    "            \"The tool will find relevant hotel reviews or travel information snippets.\"\n",
    "        )\n",
    "\n",
    "        # The main execution method for the tool.\n",
    "        def _run(self, user_query: str) -> str:\n",
    "            # Validate the input query.\n",
    "            if not isinstance(user_query, str) or not user_query.strip():\n",
    "                return \"Error: Invalid input. The user query must be a non-empty string.\"\n",
    "            \n",
    "            print(f\"[VectorDBQueryToolForCrew._run] Received query for DB search: '{user_query}'\")\n",
    "            # Call the underlying search function.\n",
    "            search_results_from_db = search_vector_db(query_text=user_query, top_k=3) \n",
    "            \n",
    "            # If no results, inform the agent.\n",
    "            if not search_results_from_db:\n",
    "                return \"No relevant information snippets were found in the database for this specific query.\"\n",
    "            \n",
    "            # Format the results into a string for the agent.\n",
    "            formatted_tool_output = \"Retrieved Information Snippets (Format: [Source, Rating, Relevance Score] Content):\\n\"\n",
    "            for i, res_item in enumerate(search_results_from_db):\n",
    "                content_snippet = res_item.get('content', 'N/A')\n",
    "                metadata_info = res_item.get('metadata', {})\n",
    "                score_val = res_item.get('score', 0.0)\n",
    "                source_info = metadata_info.get('source', 'Unknown')\n",
    "                rating_info = metadata_info.get('rating', 'N/A')\n",
    "                # Append each result with its metadata and a snippet of content.\n",
    "                formatted_tool_output += f\"Snippet {i+1}: [{source_info}, Rating: {rating_info}, Score: {score_val:.3f}] {content_snippet[:300]}...\\n---\\n\"\n",
    "            return formatted_tool_output\n",
    "\n",
    "    # Instantiate the custom tool.\n",
    "    crewai_vector_db_tool = VectorDBQueryToolForCrew()\n",
    "    print(\"VectorDBQueryToolForCrew (CrewAI tool) created successfully.\")\n",
    "\n",
    "    # Test the tool directly.\n",
    "    print(\"\\n--- Testing the CrewAI Tool directly ---\")\n",
    "    test_query_for_crewai_tool_instance = \"any good hotels in downtown with a gym and free breakfast?\"\n",
    "    print(f\"Tool Test Query: '{test_query_for_crewai_tool_instance}'\")\n",
    "    # Run the tool with the test query.\n",
    "    retrieved_info_from_tool_test = crewai_vector_db_tool.run(test_query_for_crewai_tool_instance)\n",
    "    print(f\"Tool Retrieved Info (first 600 chars):\\n{retrieved_info_from_tool_test[:600]}...\")\n",
    "\n",
    "except NameError:\n",
    "    # This error occurs if search_vector_db wasn't defined (e.g., previous cell not run).\n",
    "    print(\"CRITICAL ERROR: `search_vector_db` function from Member A is not defined. Cannot create CrewAI tool. Please ensure Member A's cells are run successfully first.\")\n",
    "except Exception as e_tool_setup:\n",
    "    print(f\"An unexpected error occurred during CrewAI tool setup: {e_tool_setup}\")\n",
    "\n",
    "\n",
    "# Cell 10: Define CrewAI Agents\n",
    "\n",
    "# Initialize agent variables.\n",
    "crewai_retriever_agent = None\n",
    "crewai_summarizer_agent = None\n",
    "crewai_composer_agent = None\n",
    "\n",
    "# Proceed only if the LLM and the custom tool are initialized.\n",
    "if llm_for_crewai and crewai_vector_db_tool:\n",
    "    print(\"\\nDefining CrewAI agents...\")\n",
    "    # Define the Retriever Agent.\n",
    "    crewai_retriever_agent = Agent(\n",
    "        role='Travel Information Retrieval Specialist',\n",
    "        goal=(\"Efficiently search the travel vector database using the 'Travel Information Vector Database Query Tool'. \"\n",
    "              \"Your sole objective is to pass the user's query to this tool and return its exact output.\"),\n",
    "        backstory=( # Provides context for the agent's persona and how it should behave.\n",
    "            \"You are an AI assistant specialized in data retrieval. You have one tool: 'Travel Information Vector Database Query Tool'.\"\n",
    "            \"When given a user query, you must use this tool by providing the query as input. Do not attempt to answer the query yourself; only use the tool.\"\n",
    "        ),\n",
    "        tools=[crewai_vector_db_tool], # List of tools available to this agent.\n",
    "        llm=llm_for_crewai, # The LLM instance this agent will use.\n",
    "        verbose=True, # Enables detailed logging of the agent's thought process.\n",
    "        allow_delegation=False, # This agent cannot delegate tasks to other agents.\n",
    "        memory=False # This agent does not have persistent memory between tasks (stateless for this run).\n",
    "    )\n",
    "    print(\"CrewAI Retriever Agent defined.\")\n",
    "\n",
    "    # Define the Summarizer Agent.\n",
    "    crewai_summarizer_agent = Agent(\n",
    "        role='Information Synthesis Expert',\n",
    "        goal=(\"Take the collection of retrieved text snippets (output from the Retriever Agent) and the original user query. \"\n",
    "              \"Identify the most critical information relevant to the query, eliminate redundancy, and produce a concise, factual summary.\"),\n",
    "        backstory=(\n",
    "            \"You are an AI assistant skilled in processing and synthesizing textual information from multiple sources. \"\n",
    "            \"You receive raw text snippets. Your task is to distill these into a coherent summary that directly addresses the user's needs as stated in their original query. Focus on facts and key opinions.\"\n",
    "        ),\n",
    "        llm=llm_for_crewai, # Uses the same LLM instance.\n",
    "        verbose=True,\n",
    "        allow_delegation=False,\n",
    "        memory=False\n",
    "    )\n",
    "    print(\"CrewAI Summarizer Agent defined.\")\n",
    "\n",
    "    # Define the Composer Agent.\n",
    "    crewai_composer_agent = Agent(\n",
    "        role='Travel Recommendation Composer and Advisor',\n",
    "        goal=(\"Generate a helpful, well-formatted, and user-friendly travel recommendation or response. \"\n",
    "              \"This response should be based *only* on the synthesized summary provided by the Summarizer Agent and should directly address the user's original query. \"\n",
    "              \"Adopt a friendly, knowledgeable travel assistant persona.\"),\n",
    "        backstory=(\n",
    "            \"You are a creative and articulate AI travel assistant. You receive a concise summary of relevant information. \"\n",
    "            \"Your responsibility is to craft this summary into an engaging and practical piece of travel advice, an itinerary suggestion, or a direct recommendation. \"\n",
    "            \"Pay close attention to tone (friendly, helpful), clarity, and formatting (e.g., use bullet points, bolding for emphasis if it enhances readability).\"\n",
    "        ),\n",
    "        llm=llm_for_crewai,\n",
    "        verbose=True,\n",
    "        allow_delegation=False,\n",
    "        memory=False\n",
    "    )\n",
    "    print(\"CrewAI Composer Agent defined.\")\n",
    "else:\n",
    "    print(\"\\nSkipping CrewAI agent definitions: LLM for CrewAI or the CrewAI Vector DB Tool is not initialized.\")\n",
    "\n",
    "\n",
    "# Cell 11: Define Main Function for Crew Execution & Demo\n",
    "\n",
    "# Main function to process a user query using the CrewAI setup.\n",
    "def get_travel_recommendation_crewai(user_query: str) -> str:\n",
    "    # Check if all agents are initialized.\n",
    "    if not all([crewai_retriever_agent, crewai_summarizer_agent, crewai_composer_agent]):\n",
    "        return \"Error: One or more CrewAI agents are not initialized. Please check the setup and logs.\"\n",
    "    # Validate the user query.\n",
    "    if not isinstance(user_query, str) or not user_query.strip():\n",
    "        return \"Error: User query is invalid (empty or not a string).\"\n",
    "\n",
    "    print(f\"\\n--- Initiating CrewAI Process for Query: '{user_query}' ---\")\n",
    "\n",
    "    # Define the Retrieval Task.\n",
    "    retrieval_task = Task(\n",
    "        description=( # Detailed description of what the agent needs to do for this task.\n",
    "            f\"A user is asking the following travel-related question: '{user_query}'. \"\n",
    "            f\"Your primary objective is to use the 'Travel Information Vector Database Query Tool' by providing it with this exact query: '{user_query}'. \"\n",
    "            f\"Ensure you extract all relevant text snippets from the database that could help answer this query.\"\n",
    "        ),\n",
    "        expected_output=( # What the successful completion of this task should produce.\n",
    "            \"A string containing several text snippets retrieved from the vector database via the tool. \"\n",
    "            \"Each snippet should be clearly demarcated. This output will be passed to the Summarizer Agent for further processing.\"\n",
    "        ),\n",
    "        agent=crewai_retriever_agent, # Assign this task to the Retriever Agent.\n",
    "    )\n",
    "\n",
    "    # Define the Summarization Task.\n",
    "    summarization_task = Task(\n",
    "        description=(\n",
    "            f\"You have received a collection of retrieved text snippets related to the user's original query: '{user_query}'. \"\n",
    "            f\"Your task is to carefully analyze these snippets, identify the most critical and relevant pieces of information, \"\n",
    "            f\"eliminate any redundancy or irrelevant details, and then synthesize a concise, factual summary. \"\n",
    "            f\"The summary must directly address the key aspects of the user's query.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"A short, coherent, and neutral summary of the most important facts and opinions extracted from the retrieved texts. \"\n",
    "            \"This summary will be used by the Composer Agent to formulate the final user response.\"\n",
    "        ),\n",
    "        agent=crewai_summarizer_agent, # Assign to the Summarizer Agent.\n",
    "        context=[retrieval_task] # This task depends on the output of retrieval_task.\n",
    "    )\n",
    "\n",
    "    # Define the Composition Task.\n",
    "    composition_task = Task(\n",
    "        description=(\n",
    "            f\"You have received a concise summary of information pertinent to the user's query: '{user_query}'. \"\n",
    "            f\"Your task is to craft a helpful, friendly, and well-formatted travel recommendation or direct answer for the user. \"\n",
    "            f\"Use only the information from the provided summary. Adopt a knowledgeable travel assistant persona. \"\n",
    "            f\"Format the response clearly, using bullet points or paragraphs as appropriate for readability.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"The final, polished, user-facing response. It should be well-written, directly answer the user's query, \"\n",
    "            \"provide practical recommendations if applicable, and be formatted for easy understanding. This is the ultimate output the user will see.\"\n",
    "        ),\n",
    "        agent=crewai_composer_agent, # Assign to the Composer Agent.\n",
    "        context=[summarization_task] # Depends on the output of summarization_task.\n",
    "    )\n",
    "\n",
    "    # Create the Crew.\n",
    "    travel_recommendation_crew = Crew(\n",
    "        agents=[crewai_retriever_agent, crewai_summarizer_agent, crewai_composer_agent], # List of agents in the crew.\n",
    "        tasks=[retrieval_task, summarization_task, composition_task], # List of tasks for the crew to execute.\n",
    "        process=Process.sequential, # Tasks will be executed one after another in the order they are listed.\n",
    "        verbose=True, # Enable verbose logging for the crew's execution. (Changed from 2 to True for boolean type)\n",
    "        memory=False # The crew itself does not maintain memory across different kickoff calls.\n",
    "    )\n",
    "\n",
    "    print(\"\\nKicking off the CrewAI travel recommendation process...\")\n",
    "    try:\n",
    "        # Start the crew's execution process.\n",
    "        crew_execution_result = travel_recommendation_crew.kickoff()\n",
    "        print(\"CrewAI process execution finished successfully.\")\n",
    "        return str(crew_execution_result) # Return the final result as a string.\n",
    "    except Exception as e_crew_kickoff:\n",
    "        # Handle any errors during the crew's execution.\n",
    "        print(f\"CRITICAL ERROR during CrewAI kickoff or execution: {e_crew_kickoff}\")\n",
    "        return f\"I'm sorry, an internal error occurred while processing your request with the AI crew. Details: {str(e_crew_kickoff)}\"\n",
    "\n",
    "print(\"`get_travel_recommendation_crewai` function defined and ready for use.\")\n",
    "\n",
    "# Demo run: Check if all necessary components are initialized before running.\n",
    "if all([crewai_retriever_agent, crewai_summarizer_agent, crewai_composer_agent, OPENAI_API_KEY, llm_for_crewai]) :\n",
    "    print(\"\\n--- Running CrewAI Demo with Example Query 1 ---\")\n",
    "    example_user_query_crew_1 = \"I want to find a quiet, charming boutique hotel in Paris, preferably in the Latin Quarter or Marais, with good reviews for cleanliness.\"\n",
    "    # Call the main function with the first example query.\n",
    "    final_response_from_crew_1 = get_travel_recommendation_crewai(example_user_query_crew_1)\n",
    "    print(f\"\\n--- Final Response for Query: '{example_user_query_crew_1}' ---\")\n",
    "    print(final_response_from_crew_1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\") # Separator for readability.\n",
    "\n",
    "    print(\"\\n--- Running CrewAI Demo with Example Query 2 ---\")\n",
    "    example_user_query_crew_2 = \"Suggest some unique cultural experiences or hidden gems in Rome for a solo traveler interested in history but wants to avoid huge crowds.\"\n",
    "    # Call the main function with the second example query.\n",
    "    final_response_from_crew_2 = get_travel_recommendation_crewai(example_user_query_crew_2)\n",
    "    print(f\"\\n--- Final Response for Query: '{example_user_query_crew_2}' ---\")\n",
    "    print(final_response_from_crew_2)\n",
    "else:\n",
    "    # If components are not ready, skip the demo.\n",
    "    print(\"\\nSkipping CrewAI demo run: One or more critical components (agents, OpenAI API key, LLM) are not initialized. Please check previous cells for errors.\")\n",
    "\n",
    "# --- END OF FILE chatgpt_commented.ipynb (Python Code) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d574a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
